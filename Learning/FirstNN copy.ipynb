{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to get my data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mypytable as mypy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AADI.csv', 'AAPL.csv', 'ABCL.csv', 'ABIO.csv', 'AACG.csv', 'AAL.csv', 'ABEO.csv', 'AAON.csv', 'AACI.csv', 'AAME.csv', 'ABCM.csv', 'ABCB.csv', 'AACIW.csv', 'ABNB.csv', 'AAOI.csv']\n"
     ]
    }
   ],
   "source": [
    "stockfiles= os.listdir(\"/home/cbarker4/Documents/DataScience/StockTrader/Data\")\n",
    "print(stockfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AADI.csv\n",
      "AAPL.csv\n",
      "ABCL.csv\n",
      "ABIO.csv\n",
      "AACG.csv\n",
      "AAL.csv\n",
      "ABEO.csv\n",
      "AAON.csv\n",
      "AACI.csv\n",
      "AAME.csv\n",
      "ABCM.csv\n",
      "ABCB.csv\n",
      "AACIW.csv\n",
      "ABNB.csv\n",
      "AAOI.csv\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y=[]\n",
    "Xlast=[]\n",
    "for val in stockfiles:\n",
    "    print(val)\n",
    "\n",
    "    mt = mypy.MyPyTable()\n",
    "    mt.load_from_file(\"/home/cbarker4/Documents/DataScience/StockTrader/Data/\"+stockfiles[0])\n",
    "    mt.drop_column('v')\n",
    "    mt.drop_column('t')\n",
    "    mt.drop_column('s')\n",
    "    i = 100\n",
    "    while i <  + len(mt.data)-1:\n",
    "        table = mt.create_sub_table(i-100,i)\n",
    "\n",
    "        table.drop_column('v')\n",
    "        table.drop_column('t')\n",
    "        X.append(table.data)\n",
    "        Xlast.append(table.data[-1])\n",
    "        Y.append(mt.get_row(i+1)[0])\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(Y)\n",
    "Y = np.array(Y)\n",
    "# print(Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.25 ,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1687, 100, 5)\n",
      "y_train shape:  (1687,)\n",
      "X_test shape:  (563, 100, 5)\n",
      "y_test shape:  (563,)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', Y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "\n",
    "    good = abs(y_true-y_pred)\n",
    "    if good>.25:\n",
    "        False\n",
    "    else:\n",
    "        True\n",
    "    \n",
    "\n",
    "\n",
    "# Build epoch checkpoint callback\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'loss',\n",
    "        patience = 7,\n",
    "        verbose = 1,\n",
    "        min_delta = 0,\n",
    "        mode = 'min',\n",
    "        baseline = None,\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(\"/home/cbarker4/Documents/DataScience/StockTrader/Model\", 'ckpt', \"{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "        monitor = 'loss',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = False,\n",
    "        mode = 'min',\n",
    "        save_freq = 'epoch',\n",
    "        options = None,\n",
    "        initial_value_threshold = None\n",
    "    )   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 28098.7871\n",
      "Epoch 1: loss improved from inf to 28098.78711, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/01-22581.60.hdf5\n",
      "43/43 [==============================] - 2s 23ms/step - loss: 28098.7871 - val_loss: 22581.6016\n",
      "Epoch 2/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 16617.2656\n",
      "Epoch 2: loss improved from 28098.78711 to 16458.31445, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/02-11597.86.hdf5\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 16458.3145 - val_loss: 11597.8564\n",
      "Epoch 3/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 7893.1821\n",
      "Epoch 3: loss improved from 16458.31445 to 7893.18213, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/03-4882.08.hdf5\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 7893.1821 - val_loss: 4882.0825\n",
      "Epoch 4/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 4025.0596\n",
      "Epoch 4: loss improved from 7893.18213 to 3992.67236, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/04-2472.40.hdf5\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 3992.6724 - val_loss: 2472.3975\n",
      "Epoch 5/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 3014.4346\n",
      "Epoch 5: loss improved from 3992.67236 to 3011.69604, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/05-2003.41.hdf5\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 3011.6960 - val_loss: 2003.4072\n",
      "Epoch 6/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2801.6838\n",
      "Epoch 6: loss improved from 3011.69604 to 2806.49731, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/06-1937.75.hdf5\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2806.4973 - val_loss: 1937.7485\n",
      "Epoch 7/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2866.4470\n",
      "Epoch 7: loss did not improve from 2806.49731\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2858.3857 - val_loss: 1905.3356\n",
      "Epoch 8/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 2807.4951\n",
      "Epoch 8: loss did not improve from 2806.49731\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2807.4951 - val_loss: 1884.7019\n",
      "Epoch 9/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 2481.7939\n",
      "Epoch 9: loss improved from 2806.49731 to 2470.44678, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/09-1129.71.hdf5\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2470.4468 - val_loss: 1129.7078\n",
      "Epoch 10/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1687.4756\n",
      "Epoch 10: loss improved from 2470.44678 to 1681.45251, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/10-482.62.hdf5\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 1681.4525 - val_loss: 482.6208\n",
      "Epoch 11/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1250.3694\n",
      "Epoch 11: loss improved from 1681.45251 to 1241.85986, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/11-327.37.hdf5\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 1241.8599 - val_loss: 327.3720\n",
      "Epoch 12/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 1143.0388\n",
      "Epoch 12: loss improved from 1241.85986 to 1147.10303, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/12-262.20.hdf5\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 1147.1030 - val_loss: 262.1965\n",
      "Epoch 13/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 1094.4694\n",
      "Epoch 13: loss improved from 1147.10303 to 1094.46936, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/13-204.74.hdf5\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1094.4694 - val_loss: 204.7426\n",
      "Epoch 14/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 1172.2498\n",
      "Epoch 14: loss did not improve from 1094.46936\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1172.2498 - val_loss: 149.4698\n",
      "Epoch 15/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1058.4407\n",
      "Epoch 15: loss improved from 1094.46936 to 1069.85315, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/15-146.99.hdf5\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1069.8531 - val_loss: 146.9932\n",
      "Epoch 16/500\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 1127.7770\n",
      "Epoch 16: loss did not improve from 1069.85315\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 1116.0609 - val_loss: 94.9332\n",
      "Epoch 17/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 1064.7784\n",
      "Epoch 17: loss improved from 1069.85315 to 1067.19775, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/17-61.86.hdf5\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1067.1978 - val_loss: 61.8588\n",
      "Epoch 18/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 1078.2842\n",
      "Epoch 18: loss did not improve from 1067.19775\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1082.3922 - val_loss: 90.1023\n",
      "Epoch 19/500\n",
      "43/43 [==============================] - ETA: 0s - loss: 1090.7496\n",
      "Epoch 19: loss did not improve from 1067.19775\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 1090.7496 - val_loss: 41.6041\n",
      "Epoch 20/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1005.6465\n",
      "Epoch 20: loss improved from 1067.19775 to 996.79364, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/20-57.29.hdf5\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 996.7936 - val_loss: 57.2944\n",
      "Epoch 21/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 973.7286\n",
      "Epoch 21: loss improved from 996.79364 to 966.62451, saving model to /home/cbarker4/Documents/DataScience/StockTrader/Model/ckpt/21-52.47.hdf5\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 966.6245 - val_loss: 52.4698\n",
      "Epoch 22/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 1011.9099\n",
      "Epoch 22: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1015.0128 - val_loss: 40.1722\n",
      "Epoch 23/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 988.6316\n",
      "Epoch 23: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 988.6010 - val_loss: 149.4780\n",
      "Epoch 24/500\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 986.0799\n",
      "Epoch 24: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 997.7622 - val_loss: 37.1940\n",
      "Epoch 25/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1083.8253\n",
      "Epoch 25: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1080.1486 - val_loss: 75.1998\n",
      "Epoch 26/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 975.6157\n",
      "Epoch 26: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 978.4279 - val_loss: 40.3142\n",
      "Epoch 27/500\n",
      "42/43 [============================>.] - ETA: 0s - loss: 1057.2207\n",
      "Epoch 27: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1057.3218 - val_loss: 56.2064\n",
      "Epoch 28/500\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 1010.0900Restoring model weights from the end of the best epoch: 21.\n",
      "\n",
      "Epoch 28: loss did not improve from 966.62451\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1007.9232 - val_loss: 29.1185\n",
      "Epoch 28: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=64,\n",
    "                            return_sequences=True,\n",
    "                            input_shape=(X_train.shape[1], 5)))\n",
    "model.add(tf.keras.layers.LSTM(units=64))\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss = 'mean_squared_error')#,metrics=accuracy)\n",
    "history = model.fit(X_train,Y_train,epochs = 500, \n",
    "                                      validation_split = 0.2, \n",
    "                                      callbacks = callbacks)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step\n",
      "[[105.02263 ]\n",
      " [228.83878 ]\n",
      " [146.18758 ]\n",
      " [182.07492 ]\n",
      " [182.07492 ]\n",
      " [228.78853 ]\n",
      " [218.7665  ]\n",
      " [185.59251 ]\n",
      " [206.54027 ]\n",
      " [212.7458  ]\n",
      " [225.15077 ]\n",
      " [147.53578 ]\n",
      " [146.18758 ]\n",
      " [166.87785 ]\n",
      " [180.04247 ]\n",
      " [198.66081 ]\n",
      " [151.2342  ]\n",
      " [105.02263 ]\n",
      " [220.0023  ]\n",
      " [145.10628 ]\n",
      " [108.05307 ]\n",
      " [106.12062 ]\n",
      " [147.58861 ]\n",
      " [ 93.25402 ]\n",
      " [146.65904 ]\n",
      " [229.11568 ]\n",
      " [126.39496 ]\n",
      " [219.33362 ]\n",
      " [222.02946 ]\n",
      " [126.79741 ]\n",
      " [183.18816 ]\n",
      " [142.81654 ]\n",
      " [108.05307 ]\n",
      " [229.11568 ]\n",
      " [105.7697  ]\n",
      " [183.18816 ]\n",
      " [188.51015 ]\n",
      " [173.71535 ]\n",
      " [141.64577 ]\n",
      " [168.49837 ]\n",
      " [220.0023  ]\n",
      " [106.14955 ]\n",
      " [141.64577 ]\n",
      " [111.04318 ]\n",
      " [167.3799  ]\n",
      " [167.76991 ]\n",
      " [221.33087 ]\n",
      " [203.6577  ]\n",
      " [173.22316 ]\n",
      " [166.87785 ]\n",
      " [140.77705 ]\n",
      " [169.40147 ]\n",
      " [209.6529  ]\n",
      " [181.66795 ]\n",
      " [147.49214 ]\n",
      " [145.10628 ]\n",
      " [226.71733 ]\n",
      " [227.57524 ]\n",
      " [227.57524 ]\n",
      " [223.12602 ]\n",
      " [219.33362 ]\n",
      " [147.51321 ]\n",
      " [151.2342  ]\n",
      " [229.05208 ]\n",
      " [146.65904 ]\n",
      " [146.18758 ]\n",
      " [106.11347 ]\n",
      " [106.14955 ]\n",
      " [ 95.25569 ]\n",
      " [209.6529  ]\n",
      " [106.11347 ]\n",
      " [220.56593 ]\n",
      " [174.69258 ]\n",
      " [168.49837 ]\n",
      " [129.3873  ]\n",
      " [147.13414 ]\n",
      " [147.13414 ]\n",
      " [228.7535  ]\n",
      " [181.20721 ]\n",
      " [204.03403 ]\n",
      " [ 95.25569 ]\n",
      " [105.41273 ]\n",
      " [125.92296 ]\n",
      " [229.18272 ]\n",
      " [147.13414 ]\n",
      " [195.02942 ]\n",
      " [202.63658 ]\n",
      " [176.19801 ]\n",
      " [147.52739 ]\n",
      " [228.8444  ]\n",
      " [228.43254 ]\n",
      " [164.68501 ]\n",
      " [126.71496 ]\n",
      " [106.77789 ]\n",
      " [145.80144 ]\n",
      " [171.32187 ]\n",
      " [106.30006 ]\n",
      " [136.50526 ]\n",
      " [106.77789 ]\n",
      " [229.18272 ]\n",
      " [208.6129  ]\n",
      " [ 87.89094 ]\n",
      " [125.48349 ]\n",
      " [106.11347 ]\n",
      " [151.2342  ]\n",
      " [142.26215 ]\n",
      " [102.33948 ]\n",
      " [224.1499  ]\n",
      " [108.05307 ]\n",
      " [146.18758 ]\n",
      " [217.45648 ]\n",
      " [170.76309 ]\n",
      " [126.90158 ]\n",
      " [229.11568 ]\n",
      " [188.51015 ]\n",
      " [140.77705 ]\n",
      " [229.1248  ]\n",
      " [126.013214]\n",
      " [167.68819 ]\n",
      " [142.81654 ]\n",
      " [173.71535 ]\n",
      " [168.49837 ]\n",
      " [166.87785 ]\n",
      " [180.04247 ]\n",
      " [204.73282 ]\n",
      " [191.04366 ]\n",
      " [198.93674 ]\n",
      " [169.40147 ]\n",
      " [228.43254 ]\n",
      " [205.04518 ]\n",
      " [228.14262 ]\n",
      " [116.61684 ]\n",
      " [226.71733 ]\n",
      " [204.73282 ]\n",
      " [208.6129  ]\n",
      " [108.05307 ]\n",
      " [200.42062 ]\n",
      " [106.11347 ]\n",
      " [167.3799  ]\n",
      " [193.02382 ]\n",
      " [228.43254 ]\n",
      " [127.403305]\n",
      " [169.8478  ]\n",
      " [170.76309 ]\n",
      " [185.59251 ]\n",
      " [220.0023  ]\n",
      " [188.51015 ]\n",
      " [198.93674 ]\n",
      " [217.45648 ]\n",
      " [116.61684 ]\n",
      " [228.7535  ]\n",
      " [141.64577 ]\n",
      " [106.30006 ]\n",
      " [167.76991 ]\n",
      " [147.53578 ]\n",
      " [145.80144 ]\n",
      " [228.83878 ]\n",
      " [182.07492 ]\n",
      " [147.52739 ]\n",
      " [188.51015 ]\n",
      " [229.18272 ]\n",
      " [214.05641 ]\n",
      " [228.7535  ]\n",
      " [209.6529  ]\n",
      " [212.7458  ]\n",
      " [227.57524 ]\n",
      " [185.59251 ]\n",
      " [106.77789 ]\n",
      " [143.2357  ]\n",
      " [210.10886 ]\n",
      " [ 93.25402 ]\n",
      " [225.45184 ]\n",
      " [147.37822 ]\n",
      " [199.08504 ]\n",
      " [108.05307 ]\n",
      " [172.56284 ]\n",
      " [141.64577 ]\n",
      " [198.66081 ]\n",
      " [144.63234 ]\n",
      " [169.8478  ]\n",
      " [105.7697  ]\n",
      " [169.16289 ]\n",
      " [151.2342  ]\n",
      " [106.30006 ]\n",
      " [223.12602 ]\n",
      " [105.41273 ]\n",
      " [229.11568 ]\n",
      " [126.601   ]\n",
      " [178.00989 ]\n",
      " [124.547295]\n",
      " [222.02946 ]\n",
      " [188.31535 ]\n",
      " [126.79741 ]\n",
      " [106.14955 ]\n",
      " [170.2194  ]\n",
      " [174.69258 ]\n",
      " [104.70806 ]\n",
      " [107.35029 ]\n",
      " [170.76309 ]\n",
      " [126.25194 ]\n",
      " [225.45184 ]\n",
      " [168.49837 ]\n",
      " [126.79741 ]\n",
      " [211.09198 ]\n",
      " [224.7195  ]\n",
      " [170.76309 ]\n",
      " [181.66795 ]\n",
      " [168.85956 ]\n",
      " [199.19777 ]\n",
      " [183.18816 ]\n",
      " [167.3799  ]\n",
      " [105.02263 ]\n",
      " [164.68501 ]\n",
      " [106.12062 ]\n",
      " [173.22316 ]\n",
      " [167.37495 ]\n",
      " [208.6129  ]\n",
      " [167.37495 ]\n",
      " [147.678   ]\n",
      " [147.52739 ]\n",
      " [210.10886 ]\n",
      " [126.22819 ]\n",
      " [185.59251 ]\n",
      " [109.809525]\n",
      " [144.21866 ]\n",
      " [106.11347 ]\n",
      " [140.77705 ]\n",
      " [ 95.25569 ]\n",
      " [226.71733 ]\n",
      " [126.25194 ]\n",
      " [105.7697  ]\n",
      " [174.69258 ]\n",
      " [217.45648 ]\n",
      " [146.18758 ]\n",
      " [211.84723 ]\n",
      " [229.1248  ]\n",
      " [189.12074 ]\n",
      " [116.61684 ]\n",
      " [172.56284 ]\n",
      " [125.92296 ]\n",
      " [146.43056 ]\n",
      " [189.12074 ]\n",
      " [210.10886 ]\n",
      " [185.59251 ]\n",
      " [205.04518 ]\n",
      " [229.24677 ]\n",
      " [147.58861 ]\n",
      " [105.7697  ]\n",
      " [191.76436 ]\n",
      " [145.49391 ]\n",
      " [108.05307 ]\n",
      " [126.38088 ]\n",
      " [126.25194 ]\n",
      " [ 93.25402 ]\n",
      " [223.12602 ]\n",
      " [106.30006 ]\n",
      " [144.21866 ]\n",
      " [126.22819 ]\n",
      " [126.27472 ]\n",
      " [108.86376 ]\n",
      " [111.04318 ]\n",
      " [181.66795 ]\n",
      " [228.83878 ]\n",
      " [105.7697  ]\n",
      " [106.12062 ]\n",
      " [147.13414 ]\n",
      " [229.05208 ]\n",
      " [203.6577  ]\n",
      " [126.22819 ]\n",
      " [227.97917 ]\n",
      " [222.02946 ]\n",
      " [172.56284 ]\n",
      " [224.7195  ]\n",
      " [227.97917 ]\n",
      " [228.43254 ]\n",
      " [147.678   ]\n",
      " [172.56284 ]\n",
      " [228.83878 ]\n",
      " [183.18816 ]\n",
      " [105.41273 ]\n",
      " [229.11568 ]\n",
      " [204.73282 ]\n",
      " [126.2572  ]\n",
      " [147.37822 ]\n",
      " [147.49214 ]\n",
      " [125.714455]\n",
      " [164.68501 ]\n",
      " [200.42062 ]\n",
      " [181.20721 ]\n",
      " [171.95013 ]\n",
      " [206.54027 ]\n",
      " [188.51015 ]\n",
      " [125.48349 ]\n",
      " [217.45648 ]\n",
      " [225.45184 ]\n",
      " [144.63234 ]\n",
      " [147.49214 ]\n",
      " [108.86376 ]\n",
      " [197.32277 ]\n",
      " [229.24677 ]\n",
      " [171.32187 ]\n",
      " [218.11295 ]\n",
      " [221.33087 ]\n",
      " [171.32187 ]\n",
      " [147.13414 ]\n",
      " [209.6529  ]\n",
      " [212.7458  ]\n",
      " [225.72188 ]\n",
      " [167.37495 ]\n",
      " [125.714455]\n",
      " [105.41273 ]\n",
      " [202.63658 ]\n",
      " [215.90819 ]\n",
      " [206.54027 ]\n",
      " [126.90158 ]\n",
      " [128.385   ]\n",
      " [178.00989 ]\n",
      " [181.66795 ]\n",
      " [197.32277 ]\n",
      " [221.33087 ]\n",
      " [126.2572  ]\n",
      " [220.0023  ]\n",
      " [106.30006 ]\n",
      " [136.50526 ]\n",
      " [197.32277 ]\n",
      " [146.65904 ]\n",
      " [195.02942 ]\n",
      " [225.15077 ]\n",
      " [126.22819 ]\n",
      " [168.02287 ]\n",
      " [170.2194  ]\n",
      " [229.05208 ]\n",
      " [220.0023  ]\n",
      " [142.81654 ]\n",
      " [143.74286 ]\n",
      " [147.37822 ]\n",
      " [193.02382 ]\n",
      " [226.71733 ]\n",
      " [212.7458  ]\n",
      " [228.78853 ]\n",
      " [168.49837 ]\n",
      " [227.97917 ]\n",
      " [198.66081 ]\n",
      " [193.02382 ]\n",
      " [144.21866 ]\n",
      " [199.08504 ]\n",
      " [143.2357  ]\n",
      " [147.13414 ]\n",
      " [170.76309 ]\n",
      " [171.95013 ]\n",
      " [126.25194 ]\n",
      " [229.24677 ]\n",
      " [182.07492 ]\n",
      " [209.6529  ]\n",
      " [126.71496 ]\n",
      " [126.27472 ]\n",
      " [167.3799  ]\n",
      " [169.8478  ]\n",
      " [229.18272 ]\n",
      " [126.27472 ]\n",
      " [173.22316 ]\n",
      " [141.64577 ]\n",
      " [168.02287 ]\n",
      " [185.59251 ]\n",
      " [212.7458  ]\n",
      " [169.16289 ]\n",
      " [116.61684 ]\n",
      " [124.547295]\n",
      " [199.19777 ]\n",
      " [173.22316 ]\n",
      " [176.19801 ]\n",
      " [105.7697  ]\n",
      " [228.98936 ]\n",
      " [210.10886 ]\n",
      " [211.09198 ]\n",
      " [200.42062 ]\n",
      " [ 95.25569 ]\n",
      " [128.385   ]\n",
      " [106.12062 ]\n",
      " [200.42062 ]\n",
      " [178.00989 ]\n",
      " [168.85956 ]\n",
      " [168.49837 ]\n",
      " [181.66795 ]\n",
      " [102.33948 ]\n",
      " [140.77705 ]\n",
      " [228.98936 ]\n",
      " [142.81654 ]\n",
      " [116.61684 ]\n",
      " [106.12062 ]\n",
      " [223.12602 ]\n",
      " [167.68819 ]\n",
      " [174.69258 ]\n",
      " [199.08504 ]\n",
      " [214.05641 ]\n",
      " [229.11568 ]\n",
      " [171.32187 ]\n",
      " [228.78853 ]\n",
      " [228.83878 ]\n",
      " [224.1499  ]\n",
      " [229.05208 ]\n",
      " [111.04318 ]\n",
      " [111.04318 ]\n",
      " [145.49391 ]\n",
      " [102.33948 ]\n",
      " [180.04247 ]\n",
      " [116.61684 ]\n",
      " [225.15077 ]\n",
      " [ 87.89094 ]\n",
      " [126.39496 ]\n",
      " [228.14262 ]\n",
      " [195.02942 ]\n",
      " [225.72188 ]\n",
      " [218.11295 ]\n",
      " [126.601   ]\n",
      " [147.52739 ]\n",
      " [136.50526 ]\n",
      " [126.2572  ]\n",
      " [189.12074 ]\n",
      " [199.08504 ]\n",
      " [142.81654 ]\n",
      " [144.63234 ]\n",
      " [173.71535 ]\n",
      " [126.39496 ]\n",
      " [ 95.25569 ]\n",
      " [106.14955 ]\n",
      " [147.49214 ]\n",
      " [125.48349 ]\n",
      " [200.42062 ]\n",
      " [106.11347 ]\n",
      " [229.18272 ]\n",
      " [198.66081 ]\n",
      " [178.00989 ]\n",
      " [198.93674 ]\n",
      " [ 95.25569 ]\n",
      " [146.18758 ]\n",
      " [111.04318 ]\n",
      " [145.80144 ]\n",
      " [205.04518 ]\n",
      " [218.11295 ]\n",
      " [164.68501 ]\n",
      " [147.53578 ]\n",
      " [126.22819 ]\n",
      " [228.8444  ]\n",
      " [173.22316 ]\n",
      " [220.56593 ]\n",
      " [106.77789 ]\n",
      " [191.04366 ]\n",
      " [147.51321 ]\n",
      " [106.11347 ]\n",
      " [125.92296 ]\n",
      " [126.2572  ]\n",
      " [136.50526 ]\n",
      " [171.32187 ]\n",
      " [212.7458  ]\n",
      " [188.23476 ]\n",
      " [129.3873  ]\n",
      " [207.0355  ]\n",
      " [ 93.25402 ]\n",
      " [164.68501 ]\n",
      " [185.59251 ]\n",
      " [215.90819 ]\n",
      " [198.93674 ]\n",
      " [218.11295 ]\n",
      " [188.31535 ]\n",
      " [126.22819 ]\n",
      " [104.70806 ]\n",
      " [140.77705 ]\n",
      " [217.45648 ]\n",
      " [229.24677 ]\n",
      " [228.14262 ]\n",
      " [227.97917 ]\n",
      " [211.09198 ]\n",
      " [169.16289 ]\n",
      " [203.6577  ]\n",
      " [228.14262 ]\n",
      " [229.24677 ]\n",
      " [116.61684 ]\n",
      " [126.2572  ]\n",
      " [169.40147 ]\n",
      " [211.09198 ]\n",
      " [223.12602 ]\n",
      " [116.61684 ]\n",
      " [142.81654 ]\n",
      " [209.6529  ]\n",
      " [126.601   ]\n",
      " [180.04247 ]\n",
      " [146.88203 ]\n",
      " [214.05641 ]\n",
      " [108.86376 ]\n",
      " [126.79741 ]\n",
      " [164.68501 ]\n",
      " [212.7458  ]\n",
      " [126.90158 ]\n",
      " [228.78853 ]\n",
      " [207.0355  ]\n",
      " [ 93.25402 ]\n",
      " [167.68819 ]\n",
      " [106.12062 ]\n",
      " [147.58861 ]\n",
      " [198.93674 ]\n",
      " [227.57524 ]\n",
      " [164.68501 ]\n",
      " [225.15077 ]\n",
      " [229.1248  ]\n",
      " [211.84723 ]\n",
      " [188.51015 ]\n",
      " [107.35029 ]\n",
      " [126.601   ]\n",
      " [207.0355  ]\n",
      " [167.76991 ]\n",
      " [143.2357  ]\n",
      " [218.11295 ]\n",
      " [106.12062 ]\n",
      " [ 87.89094 ]\n",
      " [178.00989 ]\n",
      " [102.33948 ]\n",
      " [126.39496 ]\n",
      " [106.14955 ]\n",
      " [136.50526 ]\n",
      " [228.8444  ]\n",
      " [128.385   ]\n",
      " [170.2194  ]\n",
      " [228.83878 ]\n",
      " [180.04247 ]\n",
      " [124.547295]\n",
      " [228.98936 ]\n",
      " [225.15077 ]\n",
      " [226.71733 ]\n",
      " [183.18816 ]\n",
      " [171.32187 ]\n",
      " [211.84723 ]\n",
      " [228.98936 ]\n",
      " [147.13414 ]\n",
      " [205.04518 ]\n",
      " [ 87.89094 ]\n",
      " [174.69258 ]\n",
      " [199.08504 ]\n",
      " [145.49391 ]\n",
      " [188.23476 ]\n",
      " [167.68819 ]\n",
      " [204.03403 ]\n",
      " [107.35029 ]\n",
      " [169.40147 ]\n",
      " [223.12901 ]\n",
      " [174.69171 ]\n",
      " [191.76521 ]\n",
      " [229.05144 ]\n",
      " [145.80179 ]\n",
      " [144.21786 ]\n",
      " [219.33725 ]\n",
      " [198.93869 ]\n",
      " [228.43219 ]\n",
      " [145.10704 ]\n",
      " [127.402054]\n",
      " [219.33725 ]\n",
      " [170.76268 ]\n",
      " [126.900856]\n",
      " [168.23401 ]\n",
      " [199.19983 ]\n",
      " [151.23872 ]\n",
      " [218.77003 ]\n",
      " [228.84407 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions:  109\n",
      "Incorrect predictions:  454\n",
      "Accuracy:  0.1936056838365897\n"
     ]
    }
   ],
   "source": [
    "Xlast=[]\n",
    "correct = 0\n",
    "for val in X_test:\n",
    "    Xlast.append(val[-1][0])\n",
    "\n",
    "\n",
    "for i,val in enumerate(Y_test):\n",
    "    \n",
    "    if Xlast[i] > val:\n",
    "        if Xlast[i]>y_pred[i]:\n",
    "            correct+=1\n",
    "    else:\n",
    "        if Xlast[i]<y_pred[i]:\n",
    "            correct+=1 \n",
    "\n",
    "print('Correct predictions: ', correct)\n",
    "print('Incorrect predictions: ', len(y_pred) - correct)\n",
    "print('Accuracy: ', correct / len(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57f72d97daca9169f30c9ad42adeb4d6ec91c220776b1e342e1d269fde56f1f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
